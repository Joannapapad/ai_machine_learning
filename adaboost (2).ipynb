{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=4000)\n",
    "\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3998\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list()\n",
    "for text in x_train:\n",
    "  tokens = text.split()\n",
    "  vocabulary.extend(tokens)\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "print(len(vocabulary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [06:11<00:00, 67.22it/s] \n",
      "100%|██████████| 25000/25000 [05:52<00:00, 70.90it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x_train_binary = list()\n",
    "x_test_binary = list()\n",
    "\n",
    "for text in tqdm(x_train):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_train_binary.append(binary_vector)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary)\n",
    "x_train_binary = np.array(x_train_binary).reshape(len(x_train_binary), -1)\n",
    "\n",
    "for text in tqdm(x_test):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_test_binary.append(binary_vector)\n",
    "\n",
    "x_test_binary = np.array(x_test_binary)\n",
    "x_test_binary = np.array(x_test_binary).reshape(len(x_test_binary), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node :\n",
    "    def __init__(self , checking_feature = None, isLeaf = False, category = None):\n",
    "        self.checking_feature = checking_feature\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.isLeaf = isLeaf\n",
    "        self.category = category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "import numpy as np\n",
    "import math\n",
    "class ID3 :\n",
    "    def __init__(self , features):\n",
    "        self.tree = None\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, x, y) :\n",
    "        most_common = mode(y.flatten())\n",
    "        self.tree = self.create_tree(x,y,features = np.arange(len(self.features)) , category = most_common)\n",
    "        return self.tree \n",
    "    \n",
    "    def create_tree(self,x_train,y_train,features,category) :\n",
    "        if len(x_train) == 0 :\n",
    "            return Node(checking_feature= None,isLeaf = True, category = category)\n",
    "        if np.all(y_train.flatten() ==0) :\n",
    "            return Node(checking_feature= None , isLeaf= True,category = 0)\n",
    "        elif np.all(y_train.flatten() == 1) :\n",
    "            return Node(checking_feature= None, isLeaf = True , category = 1)\n",
    "        \n",
    "        if len(features) == 0 :\n",
    "           return Node(checking_feature= None, isLeaf = True, category = mode(y_train.flatten())) \n",
    "        \n",
    "        igs = list()\n",
    "        for feat_index in features.flatten() :\n",
    "            igs.append(self.calculate_ig(y_train.flatten() , [example[feat_index] for example in x_train]))\n",
    "\n",
    "        max_ig_idx = np.argmax(np.array(igs).flatten())\n",
    "        common_category = mode(y_train.flatten())\n",
    "\n",
    "        root = Node(checking_feature= max_ig_idx)\n",
    "\n",
    "        # data subset with category = 0 \n",
    "        x_train_0 = x_train[x_train[:, max_ig_idx] == 0, :]\n",
    "        y_train_0 = y_train[x_train[:,max_ig_idx] == 0].flatten()\n",
    "\n",
    "        # data subset with category = 1\n",
    "        x_train_1 = x_train[x_train[:, max_ig_idx] == 1, :]\n",
    "        y_train_1 = y_train[x_train[:,max_ig_idx] == 1].flatten()\n",
    "\n",
    "        new_features_indices = np.delete(features.flatten(), max_ig_idx)\n",
    "\n",
    "        root.left_child = self.create_tree(x_train = x_train_1 , y_train = y_train_1, features = new_features_indices , category = common_category)\n",
    "        root.right_child = self.create_tree(x_train = x_train_0, y_train = y_train_0, features=new_features_indices,category = common_category)\n",
    "\n",
    "        return root \n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ig(classes_vector, feature):\n",
    "        classes = set(classes_vector)\n",
    "\n",
    "        HC = 0\n",
    "        for c in classes:\n",
    "            PC = list(classes_vector).count(c) / len(classes_vector)  # P(C=c)\n",
    "            HC += - PC * math.log(PC, 2)  # H(C)\n",
    "            # print('Overall Entropy:', HC)  # entropy for C variable\n",
    "            \n",
    "        feature_values = set(feature)  # 0 or 1 in this example\n",
    "        HC_feature = 0\n",
    "        for value in feature_values:\n",
    "            # pf --> P(X=x)\n",
    "            pf = list(feature).count(value) / len(feature)  # count occurences of value \n",
    "            indices = [i for i in range(len(feature)) if feature[i] == value]  # rows (examples) that have X=x\n",
    "\n",
    "            classes_of_feat = [classes_vector[i] for i in indices]  # category of examples listed in indices above\n",
    "            for c in classes:\n",
    "                # pcf --> P(C=c|X=x)\n",
    "                pcf = classes_of_feat.count(c) / len(classes_of_feat)  # given X=x, count C\n",
    "                if pcf != 0: \n",
    "                    # - P(X=x) * P(C=c|X=x) * log2(P(C=c|X=x))\n",
    "                    temp_H = - pf * pcf * math.log(pcf, 2)\n",
    "                    # sum for all values of C (class) and X (values of specific feature)\n",
    "                    HC_feature += temp_H\n",
    "        \n",
    "        ig = HC - HC_feature\n",
    "        return ig    \n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted_classes = list()\n",
    "\n",
    "        for unlabeled in x:  # for every example \n",
    "            tmp = self.tree  # begin at root\n",
    "            while not tmp.isLeaf:\n",
    "                if unlabeled.flatten()[tmp.checking_feature] == 1:\n",
    "                    tmp = tmp.left_child\n",
    "                else:\n",
    "                    tmp = tmp.right_child\n",
    "            \n",
    "            predicted_classes.append(tmp.category)\n",
    "        \n",
    "        return np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statistics import mode\n",
    "\n",
    "class AdaBoostID3:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = []  # List of ID3 trees\n",
    "        self.alphas = []  # Classifier weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, _ = X.shape\n",
    "        w = np.ones(n_samples) / n_samples  # Initialize sample weights\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sampling based on weights\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True, p=w)\n",
    "            X_bootstrap, y_bootstrap = X[indices], y[indices]\n",
    "\n",
    "            # Train an ID3 tree\n",
    "            tree = ID3(features=np.arange(X.shape[1]))  # Use all features\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            y_pred = tree.predict(X)\n",
    "\n",
    "            # Compute weighted error\n",
    "            err = np.sum(w * (y_pred != y)) / np.sum(w)\n",
    "\n",
    "            # Compute alpha (classifier weight)\n",
    "            if err == 0:  # Avoid division by zero\n",
    "                alpha = 1\n",
    "            else:\n",
    "                alpha = 0.5 * np.log((1 - err) / (err + 1e-10))\n",
    "\n",
    "            # Update sample weights\n",
    "            w *= np.exp(-alpha * y * y_pred)\n",
    "            w /= np.sum(w)  # Normalize\n",
    "\n",
    "            # Store model and alpha\n",
    "            self.models.append(tree)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        final_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            final_pred += alpha * model.predict(X)\n",
    "\n",
    "        return np.sign(final_pred)  # Convert to -1/1 labels\n",
    "\n",
    "# Train AdaBoost with ID3\n",
    "adaboost_id3 = AdaBoostID3(n_estimators=50)\n",
    "adaboost_id3.fit(x_train_binary, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = adaboost_id3.predict(x_test_binary)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Δημιουργία συνάρτησης για καμπύλες μάθησης\n",
    "def learning_curves(model, X_train, y_train, X_dev, y_dev, step_size=1000):\n",
    "    \"\"\"\n",
    "    Υπολογισμός καμπυλών μάθησης για διαφορετικά μεγέθη εκπαίδευσης.\n",
    "    :param model: Το μοντέλο AdaBoost\n",
    "    :param y_train: Ετικέτες εκπαίδευσης\n",
    "    :param X_dev: Δεδομένα ανάπτυξης\n",
    "    :param y_dev: Ετικέτες ανάπτυξης\n",
    "    :param step_size: Βήμα αύξησης του μεγέθους εκπαίδευσης\n",
    "    :return: Λίστες με ακρίβεια, ανάκληση, και F1-score\n",
    "    \"\"\"\n",
    "    training_sizes = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(step_size, len(X_train) + 1, step_size):\n",
    "        # Training subset\n",
    "        #KANE 2D\n",
    "        X_subset = X_train[:i]\n",
    "        y_subset = y_train[:i]\n",
    "\n",
    "        # Verify alignment of subset\n",
    "        assert len(X_subset) == len(y_subset), \"Mismatch between X_subset and y_subset lengths\"\n",
    "\n",
    "        # Εκπαίδευση του μοντέλου\n",
    "        model.fit(X_subset, y_subset)\n",
    "\n",
    "        # Πρόβλεψη στα δεδομένα ανάπτυξης\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        # Υπολογισμός ακρίβειας, ανάκλησης, και F1\n",
    "        precision = precision_score(y_dev, y_pred, average='binary')\n",
    "        recall = recall_score(y_dev, y_pred, average='binary')\n",
    "        f1 = f1_score(y_dev, y_pred, average='binary')\n",
    "\n",
    "        # Αποθήκευση αποτελεσμάτων\n",
    "        training_sizes.append(i)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        return training_sizes, precisions, recalls, f1_scores\n",
    "\n",
    "# Δημιουργία γραφημάτων\n",
    "def plot_learning_curves(training_sizes, precisions, recalls, f1_scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_sizes, precisions, label=\"Precision\")\n",
    "    plt.plot(training_sizes, recalls, label=\"Recall\")\n",
    "    plt.plot(training_sizes, f1_scores, label=\"F1-score\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Εφαρμογή του μοντέλου και υπολογισμός καμπυλών\n",
    "ab_classifier = AdaBoost()\n",
    "\n",
    "# Δημιουργία δεδομένων ανάπτυξης από τα δεδομένα εκπαίδευσης\n",
    "dev_size = int(0.1 * len(x_train_binary))  # 10% ως development set\n",
    "X_dev = x_train_binary[:dev_size]\n",
    "y_dev = y_train[:dev_size]\n",
    "\n",
    "X_train = x_train_binary[dev_size:]\n",
    "y_train = y_train[dev_size:]\n",
    "\n",
    "# Υπολογισμός καμπυλών μάθησης\n",
    "training_sizes, precisions, recalls, f1_scores = learning_curves(\n",
    "ab_classifier, X_train, y_train, X_dev, y_dev\n",
    ")\n",
    "\n",
    "# Σχεδίαση καμπυλών\n",
    "plot_learning_curves(training_sizes, precisions, recalls, f1_scores)\n",
    "\n",
    "# Υπολογισμός τελικών αποτελεσμάτων στα δεδομένα αξιολόγησης\n",
    "ab_classifier.fit(X_train, y_train)\n",
    "y_test_pred = ab_classifier.predict(x_test_binary_final)\n",
    "\n",
    "# Ακρίβεια, Ανάκληση και F1 για κατηγορίες και μέσα (macro, micro)\n",
    "precision_pos = precision_score(y_test, y_test_pred, pos_label=1)\n",
    "recall_pos = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "f1_pos = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "\n",
    "precision_neg = precision_score(y_test, y_test_pred, pos_label=0)\n",
    "recall_neg = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "f1_neg = f1_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "precision_macro = precision_score(y_test, y_test_pred, average=\"macro\")\n",
    "recall_macro = recall_score(y_test, y_test_pred, average=\"macro\")\n",
    "f1_macro = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "\n",
    "precision_micro = precision_score(y_test, y_test_pred, average=\"micro\")\n",
    "recall_micro = recall_score(y_test, y_test_pred, average=\"micro\")\n",
    "f1_micro = f1_score(y_test, y_test_pred, average=\"micro\")\n",
    "\n",
    "# Εκτύπωση αποτελεσμάτων\n",
    "print(\"Precision (Positive):\", precision_pos)\n",
    "print(\"Recall (Positive):\", recall_pos)\n",
    "print(\"F1 (Positive):\", f1_pos)\n",
    "\n",
    "print(\"Precision (Negative):\", precision_neg)\n",
    "print(\"Recall (Negative):\", recall_neg)\n",
    "print(\"F1 (Negative):\", f1_neg)\n",
    "\n",
    "print(\"Macro-averaged Precision:\", precision_macro)\n",
    "print(\"Macro-averaged Recall:\", recall_macro)\n",
    "print(\"Macro-averaged F1:\", f1_macro)\n",
    "\n",
    "print(\"Micro-averaged Precision:\", precision_micro)\n",
    "print(\"Micro-averaged Recall:\", recall_micro)\n",
    "print(\"Micro-averaged F1:\", f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def learning_curves_sklearn(model, X_train, y_train, X_dev, y_dev, step_size=1000):\n",
    "    training_sizes, precisions, recalls, f1_scores = [], [], [], []\n",
    "    \n",
    "    for i in range(step_size, min(len(X_train), len(y_train)) + 1, step_size):\n",
    "        #KANE 2D\n",
    "        X_subset, y_subset = X_train[:i], y_train[:i]\n",
    "        model.fit(X_subset, y_subset)\n",
    "        y_pred = model.predict(X_dev)\n",
    "        \n",
    "        precisions.append(precision_score(y_dev, y_pred, average='binary'))\n",
    "        recalls.append(recall_score(y_dev, y_pred, average='binary'))\n",
    "        f1_scores.append(f1_score(y_dev, y_pred, average='binary'))\n",
    "        training_sizes.append(i)\n",
    "    \n",
    "    return training_sizes, precisions, recalls, f1_scores\n",
    "\n",
    "def plot_learning_curves(training_sizes, precisions, recalls, f1_scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_sizes, precisions, label=\"Precision\")\n",
    "    plt.plot(training_sizes, recalls, label=\"Recall\")\n",
    "    plt.plot(training_sizes, f1_scores, label=\"F1-score\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curves - AdaBoost (Scikit-learn)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Διαίρεση σε training και development set\n",
    "dev_size = int(0.1 * len(x_train_binary))\n",
    "X_dev, y_dev = x_train_binary[:dev_size], y_train[:dev_size]\n",
    "X_train, y_train = x_train_binary[dev_size:], y_train[dev_size:]\n",
    "\n",
    "# Διασφάλιση συμβατών μεγεθών\n",
    "dev_size = min(len(X_dev), len(y_dev))\n",
    "train_size = min(len(X_train), len(y_train))\n",
    "X_dev, y_dev = X_dev[:dev_size], y_dev[:dev_size]\n",
    "X_train, y_train = X_train[:train_size], y_train[:train_size]\n",
    "\n",
    "# Εκπαίδευση και αξιολόγηση του AdaBoost του Scikit-learn\n",
    "sklearn_ab = AdaBoostClassifier(binarize=None)\n",
    "training_sizes, precisions, recalls, f1_scores = learning_curves_sklearn(\n",
    "    sklearn_ab, X_train, y_train, X_dev, y_dev\n",
    ")\n",
    "plot_learning_curves(training_sizes, precisions, recalls, f1_scores)\n",
    "\n",
    "# Τελική αξιολόγηση στο test set\n",
    "sklearn_ab.fit(X_train, y_train)\n",
    "y_test_pred = sklearn_ab.predict(x_test_binary[:len(y_test)])\n",
    "\n",
    "# Υπολογισμός μετρικών\n",
    "precision_macro = precision_score(y_test[:len(y_test_pred)], y_test_pred, average=\"macro\")\n",
    "recall_macro = recall_score(y_test[:len(y_test_pred)], y_test_pred, average=\"macro\")\n",
    "f1_macro = f1_score(y_test[:len(y_test_pred)], y_test_pred, average=\"macro\")\n",
    "\n",
    "# Εκτύπωση αποτελεσμάτων\n",
    "print(\"Scikit-learn AdaBoost Results:\")\n",
    "print(\"Macro Precision:\", precision_macro)\n",
    "print(\"Macro Recall:\", recall_macro)\n",
    "print(\"Macro F1:\", f1_macro)\n",
    "Μήνυμα στους Νεφέλη Δημητρίου\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Δημιουργία του AdaBoost με Decision Stumps (δέντρα βάθους 1)\n",
    "model = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Decision stump\n",
    "    n_estimators=50,  # Αριθμός weak learners\n",
    "    algorithm=\"SAMME\"  # Για binary classification\n",
    ")\n",
    "\n",
    "# Εκπαίδευση του μοντέλου\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Πρόβλεψη\n",
    "y_pred = model.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
