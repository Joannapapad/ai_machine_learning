{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\tsiok\\anaconda3\\envs\\numenv\\lib\\site-packages (from scikit-learn) (1.26.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/60.6 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.6 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/60.6 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.6/60.6 kB 402.7 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "   ---------------------------------------- 0.0/11.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.2 MB 2.0 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.2/11.2 MB 1.8 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/11.2 MB 2.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/11.2 MB 3.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.1/11.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.7/11.2 MB 6.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.3/11.2 MB 7.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/11.2 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.4/11.2 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.2/11.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.8/11.2 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.3/11.2 MB 10.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.9/11.2 MB 10.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.5/11.2 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.0/11.2 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.7/11.2 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.2 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.7/11.2 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.2/11.2 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.8/11.2 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.4/11.2 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.2 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.2/11.2 MB 12.8 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.8/301.8 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/46.2 MB 9.6 MB/s eta 0:00:05\n",
      "    --------------------------------------- 1.0/46.2 MB 11.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.6/46.2 MB 11.1 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.9/46.2 MB 10.7 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 2.2/46.2 MB 9.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.6/46.2 MB 9.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 3.1/46.2 MB 9.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.5/46.2 MB 9.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.9/46.2 MB 9.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 4.1/46.2 MB 9.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 4.5/46.2 MB 9.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 5.0/46.2 MB 9.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 5.5/46.2 MB 9.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 5.7/46.2 MB 9.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 6.0/46.2 MB 9.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 6.2/46.2 MB 8.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 6.7/46.2 MB 8.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 7.1/46.2 MB 9.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 7.5/46.2 MB 8.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 7.5/46.2 MB 8.5 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 8.0/46.2 MB 8.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 8.9/46.2 MB 9.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 9.5/46.2 MB 9.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 10.0/46.2 MB 9.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 10.3/46.2 MB 9.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 11.0/46.2 MB 9.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 11.5/46.2 MB 9.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 12.0/46.2 MB 9.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 12.3/46.2 MB 9.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 12.7/46.2 MB 9.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 13.4/46.2 MB 9.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 13.8/46.2 MB 9.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 14.3/46.2 MB 9.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 14.7/46.2 MB 9.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 15.1/46.2 MB 9.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 15.5/46.2 MB 9.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 15.9/46.2 MB 9.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 16.4/46.2 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 16.7/46.2 MB 9.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 17.3/46.2 MB 9.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 17.8/46.2 MB 10.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 17.9/46.2 MB 10.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 18.6/46.2 MB 10.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 19.0/46.2 MB 9.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 19.3/46.2 MB 9.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 19.6/46.2 MB 9.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.2/46.2 MB 9.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.5/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.6/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.6/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.6/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.6/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.6/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.6/46.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 21.6/46.2 MB 7.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 23.8/46.2 MB 9.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 24.4/46.2 MB 9.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 24.7/46.2 MB 9.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 25.1/46.2 MB 9.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 25.6/46.2 MB 9.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 26.0/46.2 MB 9.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 26.4/46.2 MB 9.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 26.9/46.2 MB 9.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 27.3/46.2 MB 9.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.8/46.2 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 28.4/46.2 MB 9.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 28.8/46.2 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 29.2/46.2 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 29.6/46.2 MB 9.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 30.0/46.2 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 30.3/46.2 MB 9.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 30.8/46.2 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 31.1/46.2 MB 12.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 31.5/46.2 MB 12.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 31.9/46.2 MB 11.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 32.5/46.2 MB 10.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 33.0/46.2 MB 10.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 33.5/46.2 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 33.9/46.2 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 34.5/46.2 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 35.1/46.2 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 35.5/46.2 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.0/46.2 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.4/46.2 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.8/46.2 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.3/46.2 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.7/46.2 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.2/46.2 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.6/46.2 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 39.1/46.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.5/46.2 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 40.0/46.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.5/46.2 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.9/46.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 41.4/46.2 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.9/46.2 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.3/46.2 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 42.9/46.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.3/46.2 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.6/46.2 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.1/46.2 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.4/46.2 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.8/46.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.2/46.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.6/46.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.0/46.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.2/46.2 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.2/46.2 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA FROM IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n = 100  \n",
    "k = 50   \n",
    "m = 1000\n",
    "# we load the dataset imdb and we set the train and test data \n",
    "(x_train,y_train), (x_test , y_test) = tf.keras.datasets.imdb.load_data(num_words= 4000)\n",
    "# we make a dictionary that shows the number of every word (ex. the word \"this\" refers to number 10)\n",
    "wordIndex = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# we reverse the map so that the dictionary shows the words and not the numbers\n",
    "index2word = dict((i + 3, word) for (word, i) in wordIndex.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "x_train_text = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test_text = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE THE VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for text in x_train_text:\n",
    "    tokens = text.split()\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "most_common_words = [word for word, i in vocabulary.most_common(n)]\n",
    "least_common_words = [word for word, i in vocabulary.most_common()[:-k-1:-1]]\n",
    "\n",
    "filtered_vocabulary = [word for word in vocabulary.keys() \n",
    "                       if word not in most_common_words and word not in least_common_words]\n",
    "\n",
    "filtered_vocabulary = list(filtered_vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE BINARY VECTOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [04:28<00:00, 92.98it/s] \n",
      "100%|██████████| 25000/25000 [05:40<00:00, 73.48it/s] \n",
      "100%|██████████| 25000/25000 [06:21<00:00, 65.56it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def create_binary_vectors(texts, vocabulary):\n",
    "  binary_vectors = []\n",
    "\n",
    "  for text in tqdm(texts):\n",
    "    tokens = text.split()\n",
    "    b_vector = list()\n",
    "    for vocab_token in vocabulary:\n",
    "      if vocab_token in tokens:\n",
    "        b_vector.append(1)\n",
    "      else:\n",
    "        b_vector.append(0)\n",
    "    binary_vectors.append(b_vector)\n",
    "\n",
    "  return np.array(binary_vectors)\n",
    "\n",
    "x_train_binary = create_binary_vectors(x_train_text, filtered_vocabulary)\n",
    "\n",
    "# πληροφοριακο κερδος\n",
    "mutual_info = mutual_info_classif(x_train_binary, y_train, discrete_features=True)\n",
    "# Επιλογή m λέξεων με το υψηλότερο πληροφοριακό κέρδος\n",
    "top_m_indices = np.argsort(mutual_info)[-m:]\n",
    "final_vocabulary = [filtered_vocabulary[i] for i in top_m_indices]\n",
    "\n",
    "# Μετατροπή των δεδομένων σε διανύσματα βάσει του τελικού λεξιλογίου\n",
    "x_train_binary_final = create_binary_vectors(x_train_text, final_vocabulary)\n",
    "x_train_binary_final = np.array(x_train_binary_final).reshape(len(x_train_binary_final), -1)\n",
    "x_test_binary_final = create_binary_vectors(x_test_text, final_vocabulary)\n",
    "x_test_binary_final = np.array(x_test_binary_final).reshape(len(x_test_binary_final), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Random Forest Classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node :\n",
    "    def __init__(self , checking_feature = None, isLeaf = False, category = None):\n",
    "        self.checking_feature = checking_feature\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.isLeaf = isLeaf\n",
    "        self.category = category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "import numpy as np\n",
    "import math\n",
    "class ID3 :\n",
    "    def __init__(self , features):\n",
    "        self.tree = None\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, x, y) :\n",
    "        most_common = mode(y.flatten())\n",
    "        self.tree = self.create_tree(x,y,features = np.arange(len(self.features)) , category = most_common)\n",
    "        return self.tree \n",
    "    \n",
    "    def create_tree(self,x_train,y_train,features,category) :\n",
    "        if len(x_train) == 0 :\n",
    "            return Node(checking_feature= None,isLeaf = True, category = category)\n",
    "        if np.all(y_train.flatten() ==0) :\n",
    "            return Node(checking_feature= None , isLeaf= True,category = 0)\n",
    "        elif np.all(y_train.flatten() == 1) :\n",
    "            return Node(checking_feature= None, isLeaf = True , category = 1)\n",
    "        \n",
    "        if len(features) == 0 :\n",
    "           return Node(checking_feature= None, isLeaf = True, category = mode(y_train.flatten())) \n",
    "        \n",
    "        igs = list()\n",
    "        for feat_index in features.flatten() :\n",
    "            igs.append(self.calculate_ig(y_train.flatten() , [example[feat_index] for example in x_train]))\n",
    "\n",
    "        max_ig_idx = np.argmax(np.array(igs).flatten())\n",
    "        common_category = mode(y_train.flatten())\n",
    "\n",
    "        root = Node(checking_feature= max_ig_idx)\n",
    "\n",
    "        # data subset with category = 0 \n",
    "        x_train_0 = x_train[x_train[:, max_ig_idx] == 0, :]\n",
    "        y_train_0 = y_train[x_train[:,max_ig_idx] == 0].flatten()\n",
    "\n",
    "        # data subset with category = 1\n",
    "        x_train_1 = x_train[x_train[:, max_ig_idx] == 1, :]\n",
    "        y_train_1 = y_train[x_train[:,max_ig_idx] == 1].flatten()\n",
    "\n",
    "        new_features_indices = np.delete(features.flatten(), max_ig_idx)\n",
    "\n",
    "        root.left_child = self.create_tree(x_train = x_train_1 , y_train = y_train_1, features = new_features_indices , category = common_category)\n",
    "        root.right_child = self.create_tree(x_train = x_train_0, y_train = y_train_0, features=new_features_indices,category = common_category)\n",
    "\n",
    "        return root \n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ig(classes_vector, feature):\n",
    "        classes = set(classes_vector)\n",
    "\n",
    "        HC = 0\n",
    "        for c in classes:\n",
    "            PC = list(classes_vector).count(c) / len(classes_vector)  # P(C=c)\n",
    "            HC += - PC * math.log(PC, 2)  # H(C)\n",
    "            # print('Overall Entropy:', HC)  # entropy for C variable\n",
    "            \n",
    "        feature_values = set(feature)  # 0 or 1 in this example\n",
    "        HC_feature = 0\n",
    "        for value in feature_values:\n",
    "            # pf --> P(X=x)\n",
    "            pf = list(feature).count(value) / len(feature)  # count occurences of value \n",
    "            indices = [i for i in range(len(feature)) if feature[i] == value]  # rows (examples) that have X=x\n",
    "\n",
    "            classes_of_feat = [classes_vector[i] for i in indices]  # category of examples listed in indices above\n",
    "            for c in classes:\n",
    "                # pcf --> P(C=c|X=x)\n",
    "                pcf = classes_of_feat.count(c) / len(classes_of_feat)  # given X=x, count C\n",
    "                if pcf != 0: \n",
    "                    # - P(X=x) * P(C=c|X=x) * log2(P(C=c|X=x))\n",
    "                    temp_H = - pf * pcf * math.log(pcf, 2)\n",
    "                    # sum for all values of C (class) and X (values of specific feature)\n",
    "                    HC_feature += temp_H\n",
    "        \n",
    "        ig = HC - HC_feature\n",
    "        return ig    \n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted_classes = list()\n",
    "\n",
    "        for unlabeled in x:  # for every example \n",
    "            tmp = self.tree  # begin at root\n",
    "            while not tmp.isLeaf:\n",
    "                if unlabeled.flatten()[tmp.checking_feature] == 1:\n",
    "                    tmp = tmp.left_child\n",
    "                else:\n",
    "                    tmp = tmp.right_child\n",
    "            \n",
    "            predicted_classes.append(tmp.category)\n",
    "        \n",
    "        return np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class RandomForest:\n",
    "\n",
    "    def __init__(self, n_trees=15, max_features = None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    #trains the random forest with multiple id3 trees \n",
    "    def fit(self, x, y):\n",
    "        num_samples , num_features = x.shape\n",
    "        if self.max_features == None :\n",
    "            self.max_features = int(np.sqrt(num_features))\n",
    "\n",
    "        for i in range (self.n_trees):\n",
    "            bootstrap_indices = np.random.choice(range(num_samples), size= num_samples,replace=True)\n",
    "            x_bootstrap, y_bootstrap = x[bootstrap_indices], y[bootstrap_indices]\n",
    "\n",
    "            features_indices = np.random.choice(range(num_features), size = self.max_features, replace= False)\n",
    "\n",
    "            # Train ID3 tree on bootstrap sample with selected features\n",
    "            tree = ID3(features =features_indices)\n",
    "            tree.fit(x_bootstrap[:,features_indices], y_bootstrap)\n",
    "            self.trees.append((tree, features_indices))\n",
    "\n",
    "    def predict(self,x):\n",
    "\n",
    "        predictions =[]\n",
    "\n",
    "        for tree,features_indices in self.trees:\n",
    "            pred= tree.predict(x[:,features_indices])\n",
    "            predictions.append(pred)\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        final_predictions = [mode(predictions[:, i]) for i in range(predictions.shape[1])]\n",
    "        return np.array(final_predictions)\n",
    "\n",
    "# Training the Random Forest classifier\n",
    "rf_classifier = RandomForest()\n",
    "rf_classifier.fit(x_train_binary_final, y_train)\n",
    "\n",
    "# Testing the classifier\n",
    "y_pred = rf_classifier.predict(x_test_binary_final)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train[dev_size:]\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Υπολογισμός καμπυλών μάθησης\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m training_sizes, precisions, recalls, f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_curves\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrf_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dev\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Σχεδίαση καμπυλών\u001b[39;00m\n\u001b[0;32m     78\u001b[0m plot_learning_curves(training_sizes, precisions, recalls, f1_scores)\n",
      "Cell \u001b[1;32mIn[42], line 30\u001b[0m, in \u001b[0;36mlearning_curves\u001b[1;34m(model, X_train, y_train, X_dev, y_dev, step_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_subset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_subset), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between X_subset and y_subset lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Εκπαίδευση του μοντέλου\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Πρόβλεψη στα δεδομένα ανάπτυξης\u001b[39;00m\n\u001b[0;32m     33\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_dev)\n",
      "Cell \u001b[1;32mIn[40], line 24\u001b[0m, in \u001b[0;36mRandomForest.fit\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train ID3 tree on bootstrap sample with selected features\u001b[39;00m\n\u001b[0;32m     23\u001b[0m tree \u001b[38;5;241m=\u001b[39m ID3(features \u001b[38;5;241m=\u001b[39mfeatures_indices)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_bootstrap\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeatures_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_bootstrap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees\u001b[38;5;241m.\u001b[39mappend((tree, features_indices))\n",
      "Cell \u001b[1;32mIn[25], line 11\u001b[0m, in \u001b[0;36mID3.fit\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y) :\n\u001b[0;32m     10\u001b[0m     most_common \u001b[38;5;241m=\u001b[39m mode(y\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmost_common\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree\n",
      "Cell \u001b[1;32mIn[25], line 45\u001b[0m, in \u001b[0;36mID3.create_tree\u001b[1;34m(self, x_train, y_train, features, category)\u001b[0m\n\u001b[0;32m     42\u001b[0m new_features_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(features\u001b[38;5;241m.\u001b[39mflatten(), max_ig_idx)\n\u001b[0;32m     44\u001b[0m root\u001b[38;5;241m.\u001b[39mleft_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_tree(x_train \u001b[38;5;241m=\u001b[39m x_train_1 , y_train \u001b[38;5;241m=\u001b[39m y_train_1, features \u001b[38;5;241m=\u001b[39m new_features_indices , category \u001b[38;5;241m=\u001b[39m common_category)\n\u001b[1;32m---> 45\u001b[0m root\u001b[38;5;241m.\u001b[39mright_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_features_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcommon_category\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "Cell \u001b[1;32mIn[25], line 45\u001b[0m, in \u001b[0;36mID3.create_tree\u001b[1;34m(self, x_train, y_train, features, category)\u001b[0m\n\u001b[0;32m     42\u001b[0m new_features_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(features\u001b[38;5;241m.\u001b[39mflatten(), max_ig_idx)\n\u001b[0;32m     44\u001b[0m root\u001b[38;5;241m.\u001b[39mleft_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_tree(x_train \u001b[38;5;241m=\u001b[39m x_train_1 , y_train \u001b[38;5;241m=\u001b[39m y_train_1, features \u001b[38;5;241m=\u001b[39m new_features_indices , category \u001b[38;5;241m=\u001b[39m common_category)\n\u001b[1;32m---> 45\u001b[0m root\u001b[38;5;241m.\u001b[39mright_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_features_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcommon_category\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "    \u001b[1;31m[... skipping similar frames: ID3.create_tree at line 45 (24 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[25], line 45\u001b[0m, in \u001b[0;36mID3.create_tree\u001b[1;34m(self, x_train, y_train, features, category)\u001b[0m\n\u001b[0;32m     42\u001b[0m new_features_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(features\u001b[38;5;241m.\u001b[39mflatten(), max_ig_idx)\n\u001b[0;32m     44\u001b[0m root\u001b[38;5;241m.\u001b[39mleft_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_tree(x_train \u001b[38;5;241m=\u001b[39m x_train_1 , y_train \u001b[38;5;241m=\u001b[39m y_train_1, features \u001b[38;5;241m=\u001b[39m new_features_indices , category \u001b[38;5;241m=\u001b[39m common_category)\n\u001b[1;32m---> 45\u001b[0m root\u001b[38;5;241m.\u001b[39mright_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_features_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcommon_category\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "Cell \u001b[1;32mIn[25], line 27\u001b[0m, in \u001b[0;36mID3.create_tree\u001b[1;34m(self, x_train, y_train, features, category)\u001b[0m\n\u001b[0;32m     25\u001b[0m igs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_index \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mflatten() :\n\u001b[1;32m---> 27\u001b[0m     igs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_ig\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     29\u001b[0m max_ig_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39marray(igs)\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     30\u001b[0m common_category \u001b[38;5;241m=\u001b[39m mode(y_train\u001b[38;5;241m.\u001b[39mflatten())\n",
      "Cell \u001b[1;32mIn[25], line 66\u001b[0m, in \u001b[0;36mID3.calculate_ig\u001b[1;34m(classes_vector, feature)\u001b[0m\n\u001b[0;32m     63\u001b[0m pf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(feature)\u001b[38;5;241m.\u001b[39mcount(value) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(feature)  \u001b[38;5;66;03m# count occurences of value \u001b[39;00m\n\u001b[0;32m     64\u001b[0m indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(feature)) \u001b[38;5;28;01mif\u001b[39;00m feature[i] \u001b[38;5;241m==\u001b[39m value]  \u001b[38;5;66;03m# rows (examples) that have X=x\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m classes_of_feat \u001b[38;5;241m=\u001b[39m [classes_vector[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]  \u001b[38;5;66;03m# category of examples listed in indices above\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# pcf --> P(C=c|X=x)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     pcf \u001b[38;5;241m=\u001b[39m classes_of_feat\u001b[38;5;241m.\u001b[39mcount(c) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(classes_of_feat)  \u001b[38;5;66;03m# given X=x, count C\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     63\u001b[0m pf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(feature)\u001b[38;5;241m.\u001b[39mcount(value) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(feature)  \u001b[38;5;66;03m# count occurences of value \u001b[39;00m\n\u001b[0;32m     64\u001b[0m indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(feature)) \u001b[38;5;28;01mif\u001b[39;00m feature[i] \u001b[38;5;241m==\u001b[39m value]  \u001b[38;5;66;03m# rows (examples) that have X=x\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m classes_of_feat \u001b[38;5;241m=\u001b[39m [\u001b[43mclasses_vector\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]  \u001b[38;5;66;03m# category of examples listed in indices above\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# pcf --> P(C=c|X=x)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     pcf \u001b[38;5;241m=\u001b[39m classes_of_feat\u001b[38;5;241m.\u001b[39mcount(c) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(classes_of_feat)  \u001b[38;5;66;03m# given X=x, count C\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Δημιουργία συνάρτησης για καμπύλες μάθησης\n",
    "def learning_curves(model, X_train, y_train, X_dev, y_dev, step_size=1000):\n",
    "    \"\"\"\n",
    "    Υπολογισμός καμπυλών μάθησης για διαφορετικά μεγέθη εκπαίδευσης.\n",
    "    :param model: Το μοντέλο random forest\n",
    "    :param X_train: Δεδομένα εκπαίδευσης\n",
    "    :param y_train: Ετικέτες εκπαίδευσης\n",
    "    :param X_dev: Δεδομένα ανάπτυξης\n",
    "    :param y_dev: Ετικέτες ανάπτυξης\n",
    "    :param step_size: Βήμα αύξησης του μεγέθους εκπαίδευσης\n",
    "    :return: Λίστες με ακρίβεια, ανάκληση, και F1-score\n",
    "    \"\"\"\n",
    "    training_sizes = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(step_size, len(X_train) + 1, step_size):\n",
    "        # Training subset\n",
    "        X_subset = X_train[:i]\n",
    "        y_subset = y_train[:i]\n",
    "\n",
    "        # Verify alignment of subset\n",
    "        assert len(X_subset) == len(y_subset), \"Mismatch between X_subset and y_subset lengths\"\n",
    "\n",
    "        # Εκπαίδευση του μοντέλου\n",
    "        model.fit(X_subset, y_subset)\n",
    "\n",
    "        # Πρόβλεψη στα δεδομένα ανάπτυξης\n",
    "        y_pred = model.predict(X_dev)\n",
    "\n",
    "        # Υπολογισμός ακρίβειας, ανάκλησης, και F1\n",
    "        precision = precision_score(y_dev, y_pred, average='binary')\n",
    "        recall = recall_score(y_dev, y_pred, average='binary')\n",
    "        f1 = f1_score(y_dev, y_pred, average='binary')\n",
    "\n",
    "        # Αποθήκευση αποτελεσμάτων\n",
    "        training_sizes.append(i)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return training_sizes, precisions, recalls, f1_scores\n",
    "\n",
    "# Δημιουργία γραφημάτων\n",
    "def plot_learning_curves(training_sizes, precisions, recalls, f1_scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_sizes, precisions, label=\"Precision\")\n",
    "    plt.plot(training_sizes, recalls, label=\"Recall\")\n",
    "    plt.plot(training_sizes, f1_scores, label=\"F1-score\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Εφαρμογή του μοντέλου και υπολογισμός καμπυλών\n",
    "rf_classifier = RandomForest()\n",
    "\n",
    "# Δημιουργία δεδομένων ανάπτυξης από τα δεδομένα εκπαίδευσης\n",
    "dev_size = int(0.1 * len(x_train_binary_final))  # 10% ως development set\n",
    "X_dev = x_train_binary_final[:dev_size]\n",
    "y_dev = y_train[:dev_size]\n",
    "\n",
    "X_train = x_train_binary_final[dev_size:]\n",
    "y_train = y_train[dev_size:]\n",
    "\n",
    "# Υπολογισμός καμπυλών μάθησης\n",
    "training_sizes, precisions, recalls, f1_scores = learning_curves(\n",
    "    rf_classifier, X_train, y_train, X_dev, y_dev\n",
    ")\n",
    "\n",
    "# Σχεδίαση καμπυλών\n",
    "plot_learning_curves(training_sizes, precisions, recalls, f1_scores)\n",
    "\n",
    "# Υπολογισμός τελικών αποτελεσμάτων στα δεδομένα αξιολόγησης\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_test_pred = rf_classifier.predict(x_test_binary_final)\n",
    "\n",
    "# Ακρίβεια, Ανάκληση και F1 για κατηγορίες και μέσα (macro, micro)\n",
    "precision_pos = precision_score(y_test, y_test_pred, pos_label=1)\n",
    "recall_pos = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "f1_pos = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "\n",
    "precision_neg = precision_score(y_test, y_test_pred, pos_label=0)\n",
    "recall_neg = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "f1_neg = f1_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "precision_macro = precision_score(y_test, y_test_pred, average=\"macro\")\n",
    "recall_macro = recall_score(y_test, y_test_pred, average=\"macro\")\n",
    "f1_macro = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "\n",
    "precision_micro = precision_score(y_test, y_test_pred, average=\"micro\")\n",
    "recall_micro = recall_score(y_test, y_test_pred, average=\"micro\")\n",
    "f1_micro = f1_score(y_test, y_test_pred, average=\"micro\")\n",
    "\n",
    "# Εκτύπωση αποτελεσμάτων\n",
    "print(\"Precision (Positive):\", precision_pos)\n",
    "print(\"Recall (Positive):\", recall_pos)\n",
    "print(\"F1 (Positive):\", f1_pos)\n",
    "\n",
    "print(\"Precision (Negative):\", precision_neg)\n",
    "print(\"Recall (Negative):\", recall_neg)\n",
    "print(\"F1 (Negative):\", f1_neg)\n",
    "\n",
    "print(\"Macro-averaged Precision:\", precision_macro)\n",
    "print(\"Macro-averaged Recall:\", recall_macro)\n",
    "print(\"Macro-averaged F1:\", f1_macro)\n",
    "\n",
    "print(\"Micro-averaged Precision:\", precision_micro)\n",
    "print(\"Micro-averaged Recall:\", recall_micro)\n",
    "print(\"Micro-averaged F1:\", f1_micro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
